{"posts":[{"title":"ASE18 - Delta Debugging Microservice Systems","text":"总结 提出了一种基于增量调试方法的故障定位方案 定位系统配置以及信息顺序带来的问题 特点：使用istio来进行调用和返回顺序的控制 感觉实际使用价值不大，现实过于复杂，增量的变量过多 引言 微服务复杂性 – 故障调试困难 四个维度：节点、实例、配置和交互顺序 贡献 一个基础设施平台 一个增量测试算法 增量测试 通过在变化的环境下反复执行相应的测试，可以识别出与测试失败相关的故障因素和不相关的故障因素。 即：找出导致测试错误的最小范围 过程 找最简环境：首先确认在故障环境下应该运行失败的测试用例可以在最原始的环境（即最简环境）下运行通过 找最小故障因素集合：使应该运行失败的测试用例仍然出现同样状况的失败结果，并且应该运行通过的测试用例全部通过 增量调试 维度：节点数目、实例数目、配置、交互顺序（微服务调用的执行和返回顺序） 目的：推导最小故障集合 表示：用一个向量来表示上述四个维度，每个维度含一个或多个位 具体过程：见《周翔-博士论文》 p63 本文同复旦CodeWisdem”周翔”的博士毕业论文《基于轨迹分析的微服务故障定位》第二部分，可自行参阅","link":"/2023/01/12/ASE18-Delta-Debugging-Microservice-Systems/"},{"title":"ASPLOS19 - Seer Leveraging Big Data to Navigate the Complexity of Performance Debugging in Cloud Microservices","text":"浅看了一下论文，参照中文翻译以及https://zhuanlan.zhihu.com/p/68249146 总结 主要研究领域是QoS维稳。切入方向是QoS的提取预测，以此给出伸缩扩容的建议。 做法：提出Seer方法，通过深度学习模型预测可能出现的QoS异常。主要通过对各种队列的分析，发现潜在的QoS异常。 Seer使用带有QoS违规标注并随时间收集的执行轨迹来训练一个 深度神经网络，以提示即将到来的QoS违规 缺点： 没说明各种队列是怎么映射到神经网络中的，以及训练数据中标注的情况。 队列中的数据通常为多个不同服务的请求，也没说这种关系如何在神经网络中表示 背景 系统很复杂 (这个图真是烂大街了，是个做微服务的都能用(狗头)) 过去的方法检测QOS异常具有滞后性 50s处发现异常，采取措施，直到恢复，需要180s左右。 所以希望提出一种方法，提前预测异常的发生 过去的通过资源使用率预测QoS是不准确的，有可能资源利用率一直高，比较难发现异常 图二为延迟率，颜色越亮，延迟越高。从上到下为后端到前端。可以看到，后端在0s发生延时，随着时间的推移，在250s，前端感受到延时 图一位资源利用率(CPU)，颜色越亮，资源利用率越高。可以看到前端一直很高，无法通过资源利用率分析QoS 系统架构 数据来源 NIC(network interface controller)过程中的队列，TCP/IP数据传输、epoll处理、socket读、内存过程，以及应用层的队列 框架设计 输入：队列信息、每个神经元对应一个微服务、从上到下分别为后端到前端 输出：每个结果代表一个微服务发送QoS异常的概率 实验 作者比较了使用不同网络情况下的预测效果，只使用日志、使用底层队列信息等。 Seer has now been deployed in the Social Network cluster for over two months, and in this time it has detected 536 upcoming QoS violations (90.6% accuracy) and avoided 495 (84%) of them.","link":"/2023/01/12/ASPLOS19-Seer-Leveraging-Big-Data-to-Navigate-the-Complexity-of-Performance-Debugging-in-Cloud-Microservices/"},{"title":"CSUR22 - Anomaly Detection and Failure Root Cause Analysis in (Micro)Service-based Cloud Applications A Survey","text":"总结 故障检测和根因定位的综述 当前的异常检测技术，都需要一个baseline去构建预知识。当线上执行差距过大（并发压力、环境、基础测试集构建错误等），会造成检测不准确 根因定位技术：各有千秋。基于拓扑的、概率/格兰杰假设的貌似会找出更多相关或者更接近根本的问题，相比基于trace比较的 摘要 检测故障和识别可能的根本原因，对于迅速恢复和修复应用程序至关重要 对现代多业务应用中当前可用的异常检测和根本原因分析技术进行结构化概述和定性分析 本论文结构为 异常检测技术 基于log的异常检测 基于分布式trace的异常检测 基于监控数据的异常检测 根因分析技术 基于log的 基于trace的 基于监控数据的 Intro 向微服务松耦合变化 但是难以区分单独故障还是级联故障(多个服务jiaohu)，难以检测和理解故障 目前异常检测(发现)和根因定位互相剥离，观察是否有相辅相成的工具【但目前我看到的大多异常检测和根因定位一起做】 术语 异常：服务响应时间变慢、吞吐降低or记录错误事件 根因分析与调试技术的区别：根本原因分析技术只是分析应用程序运行时收集的信息(例如，日志或监控数据)，而不是在不同的条件下重新运行它【无关紧要的区别】 监控内容：KPIs(关键性能指标), log, trace 异常检测 基于log的异常检测 无监督学习baseline，在线分析log是否与baseline是否相同 OASIS [58]: 先运行一个无错误的应用，提取log，挖掘一个控制流图，对正常条件下的基线应用行为进行建模。内含：应该由哪个服务记录哪些事件，记录的顺序是什么 然后，通过将新生成的日志映射到相应的模板，并检查到达的日志模板是否符合控制流图，实现在线异常检测。如果在相应的时间延迟中没有记录应该遵循模板的模板，或者遵循给定模板的模板的实际速率明显偏离预期的分支概率，那么OASIS认为应用程序受到了功能异常的影响。 LogSed [34]: 时间加权流程图 总结： 共同假设：类似的事件会有类似的log。 但实际上服务是动态的，可能导致检测假阳性/假阴性 基于分布式追踪数据的异常检测 trace获取 + （无监督/监督机器学习 | trace比较技术） 常见的假设是，应用程序在训练运行中产生的轨迹与运行时产生的轨迹是一致的，因此提供了一个基线，用来比较新产生的轨迹。 无监督机器学习 在生产前训练无监督网络，侦测生产过程中的问题 TraceAnomaly [45]: 如果有新数据，则检查白名单，再给出阈值表示是否异常 Nedelkoski [61]: LSTM 监督学习 标记+训练+判断 MEPFL: 复旦WisdomCode团队工作 ((20220330164333-6snty44 ‘WisdomCode-MEPFL-基于日志学习的错误预测与故障定位’)) trace对比技术 trace相似度比较。收集多服务应用程序中可能出现的轨迹，然后检查新收集的轨迹是否与已收集的轨迹相似。 缺点：应用程序继续与以前的运行保持一致的情况下再次进行的，以前收集的跟踪提供了一个基线，用于比较新收集的跟踪。因此，当应用程序的运行时条件与收集基线轨迹时不同时，制定的异常检测的准确性可能会降低。 Meng et al. [54] and Wang et al. [88]：预生成trace调用树 -&gt; 新trace计算编辑距离，如歌超过阈值，则异常; 预生成响应时间矩阵 -&gt; 主成分分析。 缺点：计算复杂度 Chen et al. [17]: 使用fast matrix sketching algorithm做异常检测，优点：快 基于监控信息的异常检测 需要在无错误模式训练，然后在生产环境中定位问题。当应用程序运行时条件有所不同,检测的准确性可能减少,例如:由于高负荷由最终用户请求的山峰,或者由于新服务部署到取代过时的 监控数据 SLI，全名Service Level Indicator，是服务等级指标的简称，它是衡定系统稳定性的指标。一个常见的SLI是请求得到正常响应的百分比。 SLO，全名Sevice Level Objective，是服务等级目标的简称，也是稳定性目标,是围绕SLI构建的目标, 比如，月度、季度、年度等。 无监督学习 学习他们行为的基线模型。然后使用基线模型在线检测服务上新监控的kpi是否偏离基线模型 MicroRCA[92]:并不算无监督，相当于图建模 ((20220510163942-ddm9aru ‘NOMS20-MicroRCA-Root Cause Localization of Performance Issues in…’)) CloudRanger[87]: 应用程序的前端服务上实施持续学习，以根据监控的响应时间、负载和吞吐量，了解当前条件下的预期响应时间。 基于历史初始模型的脱机训练 当应用程序运行时，CloudRanger对监控的kpi应用多项式回归来更新基线模型，以反映应用程序在动态变化的运行时条件下的行为。 然后，根据给定的容错阈值，通过检查应用程序前端的响应时间是否明显偏离预期时间来实施在线异常检测。 监督学习 正常运行、注入特定的故障，生成训练集 SLO检测 ϵ-diagnosis [76]: 注重尾延迟 Heartbeating 根因定位 基于log的根因定位 建模，then确认异常 Aggarwal et al. [1]: 因果关系图的顶点对应用程序服务进行建模，而因果关系图的有向弧则对源服务中的异常可能导致目标服务中的异常进行建模; 时间序列建模，计算格兰杰因果关系，确定错误记录之间的因果性 随机游走确定根因 基于trace的根因定位 方法：①可视化 ②统计分析 ③拓扑分析 可视化 Zhou et al. [98]: ((20211126002021-vogdt0p ‘ITSE21-Fault Analysis and Debugging of Microservice Systems: Ind…’)) 统计分析 CloudDiag [56]: 变异系数, 阈值判断。基于主成分分析处理 TraceAnomaly [45]: 神经网络，根据执行时间，把trace分为正常异常。每个响应时间与平均响应时间有显著偏差的服务交互都被认为是异常的 基于图 MonitorRank [38]: 基于图的随机游走 MicroHECL [44]： BFS, 找到异常, 用某种方法遍历所有节点，看节点传播能否到达异常 基于监控数据的根因定位 大多数基于监视的根本原因分析技术都依赖相关性作为驱动因素，以识别所观察到的异常的可能根本原因。其思想是，在异常服务上监控的kpi与在其他服务上并行监控的kpi之间的相关性越高，后者导致在前者上观察到的异常的可能性就越大 然而，考虑到相关性并不能确保因果关系，服务中观察到的异常的实际根本原因可能不在其监控kpi与异常服务高度相关的服务中，而在其他服务中。因此，伪相关性会影响利用KPI相关性进行统计分析或驱动拓扑/因果关系图中查找根本原因的技术。这增加了假阴性的风险，通常通过返回多个可能值来解决次问题 统计分析 Wang et al. [86]: 通过识别其监控kpi与检测到的异常同时异常的应用程序服务，可以确定服务上检测到的异常的根本原因 PAL [63]: CUSUM(累积和)图度量监视的KPI值的变化幅度，包括监视值的原始序列和自举序列，而自举序列是对所监视的KPI值进行随机排序的排列。如果原始序列的变化幅度高于大多数bootstrap，则认为原始序列是异常的，并根据其CUSUM图确定异常开始的时间。减少误报：为了减少误报，PAL和FChain检查异常是否影响了所有应用程序服务，在这种情况下，异常被归类为外部原因(如工作负载峰值)。相反，如果只有应用程序服务的子集异常，则认为它们是前端异常的可能根源，并按照异常开始时间的顺序返回它们。错误定位：最早的异常确实被认为是最有可能的根本原因，因为它们可能已经从相应的服务传播到其他服务，直到导致观测到的前端异常。 基于图的分析 randomwalk：MicroRCA 其他方法，格兰杰因果测试等 因果关系图分析 PC-algorithm BFS Random Walk 原理总结 因果图中的顶点要么为应用程序[18,19,24,42,43,48,50,87]建模，要么为这些服务上监视的KPI[51, 55, 68]建模，而每个弧线表示目标服务/KPI的性能取决于源服务/KPI的性能。 因果关系图总是通过对日志事件/监控kpi的时间序列应用pc -算法[82]来获得(但对于LOUD[51])，因为已知该算法允许确定时间序列中的因果关系，即:通过确定一个时间序列中的值是否可以用来预测另一个时间序列的值 搜索方法：从异常节点(服务/kpi)出发，BFS或者Random Walk randomwalk: 访问具有随机步长服务的概率与该服务的性能与观察到异常的服务之间的相关性成正比 讨论 成本 精度 假阳性解决方法 基线模型：不断训练新的模型 trace解决方法：将新观察到的信息和之前的所有信息进行比较 可解释性和对策","link":"/2023/01/13/CSUR22-Anomaly%20Detection%20and%20Failure%20Root%20Cause%20Analysis%20in%20(Micro)Service-based%20Cloud%20Applications%20A%20Survey/"},{"title":"ESECFSE20-GMTA-Graph-based trace analysis for microservice architecture understanding and problem diagnosis","text":"异常检测+图形化表示 应用于eBay的一套系统 设计了快速的存储、日志处理等模块 主要是一套展示系统","link":"/2023/01/12/ESECFSE20-GMTA-Graph-based-trace-analysis-for-microservice-architecture-understanding-and-problem-diagnosis/"},{"title":"FSE19-MEPFL-Latent Error Prediction and Fault Localization for Microservice","text":"总结 使用机器学习，基于日志记录，进行错误预测和故障定位 机器学习：分类算法。日志扔进去，分类为正确或错误，属于什么错误 训练集：多种错误分别注入，日志收集，对应有故障标签，训练 缺陷：首先要有针对某个服务的训练集，其次错误类型相比实际应该很小，跑的时候不一定分的出来 Intro 提出了基于执行轨迹日志机器学习的微服务潜在错误与故障根源预测方法 故障类型： 多实例故障、系统配置故障、异步交互故障 MEPFL 预测目标 轨迹层级：潜在错误（T/F)、故障微服务、故障类型，微服务层级：微服务的故障类型（使用一组微服务运行时特征预测） 过程： 离线训练+在线预测 预测模型： 特征定义 轨迹层级 微服务系统层级： 每个服务的RSC、轨迹执行时间、轨迹涉及微服务数、轨迹实例中涉及微服务实例数 模型设计 四个分类模型 训练方法：随机森林、K邻近、多层感知机 训练集（故障注入） 多种错误分别注入，日志收集，对应有故障标签，训练 本文同复旦CodeWisdem”周翔”的毕业论文《基于轨迹分析的微服务故障定位》第三部分，可自行参阅","link":"/2023/01/12/FSE19-MEPFL-Latent%20Error%20Prediction%20and%20Fault%20Localization%20for%20Microservice/"},{"title":"FSE20-Intelligent REST API Data Fuzzing","text":"要点：fuzzing，REST API，测试总结 智能地生成嵌入在REST API请求中的数据负载 生成方法 schema fuzzing规则 fuzzing 规则的组合方法 搜索方法（对请求body每个值fuzzing的顺序） 从swagger、响应、example中提取数据值 评估标准：某种fuzzing方法能出发error的类型数量，数量越多，越有效 Intro fuzzing: automatic test generation and execution with the goal of finding security vulnerabilities. 目的：评估多种fuzzing技术(对rest api请求的fuzzing) Background 这篇文章研究了如何fuzzing有状态的rest API, (尤其相对于RESTler而言）。通过智能fuzzing，生成更复杂的数据，找到更多的服务器错误 Schema Fuzzing - 最基本的表单fuzzing fuzzing规则 – 对请求body tree的fuzzing Node fuzzing rules Drop Select Duplicate Type Tree fuzzing 规则 single: fuzzing一个节点 path：选一条路径fuzzing all 实验 实验评估 错误信息 error code error message error type = code+message 实验设置 实验了以上12种组合，4个node*3个tree规则 每条请求有最多1000个fuzzing点 | 一个请求最多fuzzing 1000次 结果 对于 drop和select而言，path比随机更好，随机会空间爆炸，path效率更高 type和duplicate都可能引起反序列化错误 每种fuzzing都有自己发现的独有错误 path最合适来fuzzing，不引入反序列化错误且有效率 Combining Schema Fuzzing Rules – fuzzing结合规则 基本规则 依次使用drop、select、duplicate、type对tree节点依次fuzzing，drop出结果后，再使用select对结果fuzzing，… 组合：四选二、四选三、四选四 fuzzing次序 BFS DFS Random 结论 drop、select、type结合是很有效的，尤其是drop、select在type前fuzzing duplicate没什么用 RD最有效 Data Value Rendering - 根据数据值fuzzing 当前challenges：无效值会被拒绝，例如global却填入europ 缺少客户端信息，例如ID、group name 缺少domain信息，例如local、global字段或者时间 缺少运行时依赖信息，后一个请求依赖于前一个 信息来源（rendering策略） 静态 type-value 字典 从swagger提取的样例 从响应中匹配字段 两种匹配策略：conservative（父节点和当前节点类型相同），aggressive（当前节点类型相同） render策略 Baseline (BAS): 字典中选择 Examples only (EXM): example选择，然后字典选择 Responses only (conservative) (CON):保守模式选择，然后字典选择 Responses only (aggressive) (AGG)：aggressive选择，然后… Responses (conservative) and examples (CON+EXM) Responses (aggressive) and examples (AGG+EXM) 结论 swagger样例能带来更多的错误类型覆盖 为叶子结点使用响应值fuzzing可以显著提高效果，尤其是激进模式 BUG HUNTING IN CLOUD SERVICES - 对云服务器的实验 实验参数设置 Each phase has a budget of 10 times the number of nodes in the schema 相关工作讨论","link":"/2023/01/12/FSE20-Intelligent-REST-API-Data-Fuzzing/"},{"title":"ICPE20-Detecting Latency Degradation Patternsin Service-based Systems","text":"总结 目标：检测异常延时 提出自动化检测与请求延时相关的RPC请求时间 – 延迟退化模式 本质上是延迟请求聚类 动态规划+遗传算法 但实际上，只需要关注某个请求变慢即可。或者用一个聚类算法一下处理getProfile和gerCart，所以本文没什么用感觉 1. Intro 在rpc执行时间中，这些复杂的非确定性行为会产生特定的模式，在本文中我们将其称为延迟退化模式 延迟降低模式的一个粗略的例子是“当getProfile执行时间超过x，而getCart执行时间超过y时，主页请求变慢”","link":"/2023/01/12/ICPE20-Detecting-Latency-Degradation-Patternsin-Service-based-Systems/"},{"title":"ICSE19-RESTler Stateful REST API Fuzzing","text":"关键词：fuzzing，swagger，响应，rest api 重点：有状态的rest api fuzzing 根据swagger推断关系（先后、生产者消费者） A请求返回a对象，B请求使用a对象作为参数，则A应当在B之前执行 根据响应动态反馈 了解到“请求序列a之后的请求C;请求B被服务拒绝”，在将来避免这种组合 摘要 restler: 有状态的rest api fuzzing方法/工具 分析服务的api描述，据此fuzzing fuzzing依据 推断规范中声明的请求类型之间的生产者-消费者依赖关系 分析从先前测试执行期间观察到的响应的动态反馈，以便生成新的测试 INTRODUCTIONPROCESSING API SPECIFICATIONS 后面有时间再补","link":"/2023/01/12/ICSE19-RESTler-Stateful-REST-API-Fuzzing/"},{"title":"ICSE22-DeepTraLog: Trace-Log Combined Microservice AnomalyDetection through Graph-based Deep Learning","text":"总结 提出log和trace的混合服务异常检测方法 基于图深度学习，不需要预训练模型，线上分类检测异常 解决两个痛点 基于log的异常检测：现有的日志异常检测方法将日志视为事件序列，无法处理分布在具有复杂交互的大量服务中的微服务日志。（事件只针对于当前服务，没有上下调用关联） 基于trace的异常检测：忽略了由调用层次结构和并行/异步调用带来的复杂的结构。（主要还是对并行请求的处理） 提出DeepTraLog: 一种基于深度学习的微服务异常检测方法。DeepTraLog使用统一的图表示来描述跟踪的复杂结构，同时结构中嵌入日志事件。结合trace和log训练GGNNs(基于deep SVDD)模型 摘要 基于log和基于trace的问题 介绍DeepTraLog Intro 基于trace的异常检测: 没有处理并行，以及log中的信息 基于log的异常检测：使用日志+时间戳为每个节点日志排序，但缺乏调用链组成信息（不认同，带traceId即可） 提出DeepTraLog 使用一种统一的图表示，称为跟踪事件图(trace event graph, TEG)，来描述跟踪的复杂结构，以及该结构中嵌入的日志事件。 它以轨迹和日志作为输入，训练基于图的深度学习模型用于轨迹异常检测 过程 首先，它解析输入跟踪和日志，并分别从中提span关系和日志事件。 其次，它为span事件和log事件生成向量表示，同时为每个trace构造一个TEG。 第三，它训练一个门控图神经网络(ggnn)基于深度SVDD(支持向量数据描述)模型，学习每个TEG的潜在表示和最小化的数据封闭超球。 当用于异常检测时，DeepTraLog以类似的方式分析跟踪和相关日志，并使用训练过的模型生成跟踪的潜在表示。然后，它根据其异常分数(即从迹的潜在表示到超球的最短距离)确定迹是否异常。 注意：不依赖于标签，实时训练 贡献： 一个统一的结合trace和log的图表示 一个基于deep SVDD的GGNNS模型用来异常检测 背景 日志：日志消息是一个非结构化的句子，它包含一个固定的部分。注意：有固定的结构 motivation log问题（同上） trace问题（同上） APPROACH Log Parsin：log日志转化为事件 日志解析，日志中附加traceID和spanID 使用Drain算法(在线日志解析，讲原本结构转化为目标结构 Trace Parsing：trace解析为span，同时解析为同步和异步 同步：clent/server 异步：生产者/消费者 一个span，涉及父span的调用、接收。子span的调用接收，共四个事件 Event Embedding：事件嵌入 log日志预处理，删除动词、单复数变幻 词向量化为, 每个词生成300维的向量(已有算法) 句子向量化, 为每个词生成权重。句子被向量化为权重的和，再归一化 类似的操作向量化span事件 Graph Construction：图构建 事件关系：同步请求、同步响应、异步请求 日志事件连接。对于span的每个范围，获取属于该范围的所有日志事件。然后，根据时间戳对日志事件进行排序，并将每个日志事件的序列关系添加到与其相邻的日志事件(时间关系，非并行关系) log日志并入span event 关系处理：client的request和response的request添加同步关系 Model Training: 一分类问题 深度SVDD通过包含数据潜在表示的最小化超球来学习训练数据的有用特征表示。因此，远离超球中心的数据可以被认为是反常的。 因此，我们使用门控图神经网络(ggnn)学习跟踪表示，并使用深度奇异值映射(SVDD)联合训练ggnn。 损失函数：最小化超球来包含TEGs的向量 Anomaly Detection: 异常检测 异常分数定义为TEG的潜表示到被学习的超球的最短距离","link":"/2023/01/12/ICSE22-DeepTraLog-Trace-Log-Combined-Microservice-AnomalyDetection-through-Graph-based-Deep-Learning/"},{"title":"ICSESEIP21-MicroHECL- MicroHECL High-Efficient Root Cause Localization in Large-Scale Microservice Systems","text":"总结 作者认为现有方法不行 现有基于图(服务依赖图)的方法，检测不准确，效率低 现有基于trace分析的方法，需要大量计算，做不到 提出一种动态图构建的方法，从可观测异常服务起，根据三种异常，扩展构建异常相关图。对图节点遍历排序，找出相关根因 摘要 现有方法对异常检测不准确以及对服务依赖图遍历低效 基于动态构造的服务调用图，分析可能的异常传播链，基于相关性分析对根因进行排序 结合机器学习和统计方法，设计定制模型检测不同类型的服务异常（性能、可靠性、流量）设计剪枝消除异常传播中不相干的服务调用 Intro 微服务系统特点 动态+复杂 服务有众多实例调用链异步交互 异常沿着调用链传播，最终导致业务问题 过去的工作 基于trace分析：基于跟踪分析的方法需要昂贵的跟踪数据收集和处理，因此不能有效地用于大规模系统。 基于依赖图分析:基于服务依赖图的方法基于服务调用和因果关系(例如，服务位于同一台机器上)构建服务依赖图。这些方法通过遍历服务依赖关系图并通过服务的质量度量(例如，响应时间)检测可能的异常来定位根本原因。这些方法的局限性在于，它们对服务异常的检测不准确，对服务依赖关系图的遍历效率低下，尤其是当系统有许多服务和依赖关系时。 背景 三种异常 性能异常：RT-响应时间 可用性异常：EC-error counts 流量异常：QPS-queries per second 整体流程 构建服务调用图 重点为图结构以及数据如何存储 异常传播链分析 分析方法是沿着异常服务调用边缘遍历服务调用图，从初始异常服务开始，沿着可能异常传播方向的相反方向进行。 用了一个剪枝方法 候选根因排序 初始异常：业务异常(订单若干次不成功) 根因异常：RT, EC, QPS 使用皮尔逊相关系数计算异常之间的相关性，并排序 异常传播链分析(上文第二点) 性能异常、可靠性异常：下游-&gt;上游流量异常: 上游-&gt;下游 流程 入口点分析，对每种异常分析 异常节点扩展，上下游检测 得到候选根因 节点异常检测 - 检测节点是否存在异常 性能异常-RT： 有周期性波动，选择OCSVM(一种支持向量机) 可靠性异常-EC: EC可能随着流量变化(流量大就G，流量小又恢复),所以选择随机森林 流量异常 3-sigma原则 剪枝 两个连续的边在对应的度量中有相似的变化趋势 使用相关系数判断，如果相关系数低于阈值，则新边不被加入","link":"/2023/01/12/ICSESEIP21-MicroHECL-MicroHECL-High-Efficient-Root-Cause-Localization-in-Large-Scale-Microservice-Systems/"},{"title":"Localization of Operational Faults in Cloud Applications by Mining","text":"介绍 为什么选日志：跟踪和基于度量的方法分别需要代码和度量的插装，因此增加了额外的开销，可能并不总是可行的。另一方面，日志在任何分布式系统中都很容易获得，这使得基于日志的方法比基于跟踪和基于度量的方法更实用 中心要点 黄金信号：用户可观测到的错误。以此为中心进行分析。黄金信号其实有延时、错误、流量饱和，此处只关心错误 分析过程 基础知识：日志建立时间序列模型，方便计算异常之间关系 流程 一个节点重复产生错误，被定为为黄金信号 构图，红色为所有打印异常/错误的节点 相关度计算：计算其他异常和黄金信号的关系(格兰杰因果关系) 异常排序：PageRank","link":"/2023/01/12/ICSOC20-Localization%20of%20Operational%20Faults%20in%20Cloud%20Applications%20by%20Mining%20Causal%20Dependencies%20in%20Logs%20using%20Golden%20Signals/"},{"title":"ISSTA15 - GA-Prof - Automating Performance Bottleneck Detection usingSearch-Based Application Profiling","text":"摘要 提出一种检测性能问题的方法，针对基于搜索的输入敏感的应用 GA-Prof: Genetic Algorithm-driven Profiler GA-Prof有效地搜索了输入值组合的大空间，同时主动和准确地检测性能瓶颈，从而表明它是有效的自动分析 Intro 探索性随机性能测试寻找瓶颈 传统分析的缺点：输入过多，流程分支复杂，难以设定好的初始值来获得瓶颈 传统工作方法： 插桩 收集数据 理解AUT的代码，理解哪些方法和特定的输入相关 在AUT中构造不同的输入组合，寻找瓶颈 对不同的输入组合，分析执行轨迹，归纳结果 工作：使用遗传算法作为搜索启发式，获得输入参数的组合，最大化指导搜索过程的适应度函数 背景 输入敏感Profiling 基于所有输入数据都是预先可用的假设。但如果瓶颈和输入的大小成正交，则简单的输入导致智能运维困难 瓶颈数据分析 某个方法在多个AUT中被调用。在每个单独的trace中，方法的总执行时间可能不会将其放在瓶颈列表的顶部，然而，当跨不同的跟踪进行分析时，根据它们对总执行时间的总体贡献，这些方法可能被视为瓶颈 [?] 通用瓶颈：比如官方math库 另一方面，特定的输入瓶颈难以找到源头，搜索空间很大 问题陈述 输入空间太大，少部分才能出发瓶颈 有些输入参数是无效的 后面有时间再补","link":"/2023/01/12/ISSTA15-GA-Prof-Automating-Performance-Bottleneck-Detection-usingSearch-Based-Application-Profiling/"},{"title":"ITSE21-Fault Analysis and Debugging of Microservice Systems: Industrial Survey, Benchmark System,and Empirical Study","text":"微服务系统的故障分析和调试： 行业调查，基准系统和经验研究 注：本文网络上有中文译文 摘要 已有微服务故障分析和调试的研究有限 做了如下工作 进行一项行业调查，了解微服务的典型故障和调试挑战 开发一个中等规模的微服务benchmark，复制了22个行业故障案例 实证研究，调查现有行业调试的有效性 提出调试的一些改进方法 结果 结果表明，通过采用适当的路径跟踪和可视化技术与策略，可以改善微服务调试的当前行业实践 关键词 微服务， 故障定位， 路径跟踪，可视化，调试 引言 迫切需要解决架构挑战，例如处理异步通信，级联故障，数据一致性问题，发现和微服务认证 理解和调试分布式系统的基本有效方法是路径跟踪和可视化系统执行。 背景 微服务的独特特性给现有的调试技术带来了挑战 现有的调试技术，断点，难以应用于微服务 现有的故障定位技术，基于切片或者频谱 行业调查 来自 12 家公司的 16 名参与者带来了他们正在或已经在使用的 13 种微服务系统的反馈 故障案例 22个故障案例 功能故障会引发错误或产生错误的结果 非功能性故障会影响服务质量，例如性能和可靠性。 内部故障、交互故障 调试 调试步骤 IU、ES、FR、FI、FS、FL 调试技术 日志分析 可视化日志分析 可视化路径跟踪分析 – Dynatrace和Ziplin… 基准系统和故障案例复制 Trainticket 复制以上22个故障案例到trainticket中 实证研究 使用以上调试技术进行调试 定性分析 定量分析 对每个错误进行发现调试，统计每种调试技术的每个调试步骤的花费时间 通过改进的路径跟踪可视化进行调试 通过将路径跟踪日志转换为 ShiViz 的日志格式，利用 ShiViz 来可视化微服务的路径跟踪 ShiViz：路径可视化调试工具 提出两种可视化策略 微服务作为节点（服务级别分析） 服务状态作为节点（状态界别分析） – 基于预定义的变量、表达式确定 提出新的调试方法 路径对比调试 原理：错误的路径与成功路径中的不同部分，与另一个故障跟踪共享 结果 进一步提高调试效率","link":"/2023/01/12/ITSE21-Fault-Analysis-and-Debugging-of-Microservice-Systems-Industrial-Survey-Benchmark-System-and-Empirical-Study/"},{"title":"NOMS20-MicroRCA-Root Cause Localization of Performance Issues in Microservices","text":"总结 使用属性图来模拟微服务系统的异常传播 使用图和权重、PageRank的方法来定位问题 已知的性能问题表现在权重上（异常症状和资源关联） PageRank对可能的问题节点进行排序，很新颖 能够更准确得定位问题，能够从症状明显的服务和症状不明显的服务中准确得找出根因，这是别的方法做不到的地方（作者言） 权重、图的数据来源仍然是默认的调用信息、调用结构、时间等 MicroRCA: 将性能症状于相关资源利用率相关联，推断出根本原因 Intro: 微服务挑战 复杂的依赖（很多服务）2. 大量的监控数据 3. 异构的服务 4. 频繁的更新 提出MicroRCA 收集应用（响应时间）和系统层级(cpu、内存）的数据，检测服务级别(service)的异常 一旦检测到，构造包含服务和host的属性图（不仅包含异常路径，还包含host上的所有服务） MRCA将异常和资源利用率相关，并排序根因 优点：通过业务异常与资源利用率的相关性，MicroRCA可以识别出异常且服务异常症状不明显的非计算密集型服务，减轻虚警对定位根本原因的影响。 贡献： 提出属性图来模拟微服务系统的异常传播 提出一种方法，能够将异常症状和资源关联，从而定位问题 总览 MicroRCA 检测到异常 -&gt; 构建相关服务和节点的属性图 -&gt; 构建子图 -&gt; 排序问题 数据收集： 服务网格+系统监控 异常检测 无监督学习： Distance-Based online clustering BIRCH 检测慢响应时间作为异常 根因定位三步曲：属性图构建、异常子图构建、错误服务定位 根因定位 属性图构建 构造一个属性图来表示微服务环境中的异常传播 选中的服务：不仅包含错误路径涉及的服务（异常在同一节点的服务间扩散） 基础架构：正常运行时的框架 异常子图构建 s1-&gt;s2: s2为异常节点 节点附加属性：平均异常响应时间rt_a 异常服务定位 edge 权重赋值 权重1：异常程度，越异常越大 权重2：异常节点对正常节点的影响，corr(t(Ei,j), t(rt_a(i))) 权重3：异常响应时间和主机资源利用率的最大相关度 最后一项为入度为异常的所有边的平均值 共有三个属性 服务异常打分 节点平均权重 * 最大[资源和异常延时之间的相关系数] 定位 PageRank v: 最终排序 p: 移动概率（根据权重计算） 1-c: 继续传播， c: 随机跳跃 u: 节点的异常分数 实验 两个指标 PRk：topk包含真实故障的概率，k越小，精度越高 平均PRk: 整体性能 实验看来，top1大多数情况能找到问题","link":"/2023/01/12/NOMS20-MicroRCA-Root-Cause-Localization-of-Performance-Issues-in-Microservices/"},{"title":"NSDI11-Spectroscope-Diagnosing performance changes by comparing request flows","text":"总结 比较两次执行（系统修改前后）的请求流（时间、结构体）发现问题；对时间方面，使用假设检验，检查某种分类的时间分布是否和之前一致，从而判断是否一致。对于结构，寻找变异及其对应的前体，来发现性能改变 设计的算法可以确定和排序请求流/时间中的改变 工具 Spectroscope 假设背景：我们的技术假设性能变化是由系统变化引起的(代码变化、配置变化等)。需要对比变化前后系统的执行来进行前体/编译的区分。 可否应用于性能问题？[Q] 定义两种突变情形 响应时间突变 结构体突变 执行过程 分组（聚类） 区分突变和前体（KS和阈值） 突变和前体关联 排序 定位low-level difference 相比TPROF， 都是聚类分组算法，聚类分组才能比较， TPROF有多种聚合层次，多了一个subspan，剩下的就是时间和结构体 本文分为时间和结构体 TPROF如何分类前体和突变[Q] 本文提出了low-level参数 1. Intro 性能变化通常表现为请求服务的变化 突变 - 问题期间新的请求流 前体 主要工作 识别突变，根据贡献度排序，高亮最显著的分歧，定义最有可能导致问题的low-level参数 突变分类 响应时间突变：结构相同，响应时间不同 结构突变：不同路径，需要找到其原请求流然后定位根因 3. 行为改变和异常检测 本文注重两个时间段进行比较 异常检测(pinpoint)注重于找到一个集合中异常的部分 4. Spectroscope 4.1 分组 按照结构分组（也就是字符串遍历，DFS） 结构分组依据依据： 类似的path会有相同的cost 结构相同，字符串遍历相同 对每个分类统计请求数、平均响应时间、方差，边缘延时及其方差 聚类算法的优缺点 聚类算法用来减少分出来类别的数量，减少开发者需要查看的类型。直接用的聚类会把突变前体（新版本中有两者）分到一起，掩盖了突变的存在 无监督聚类算法 – Magpie 4.2 比较请求流 输入：有问题阶段和无问题阶段使用统计测试和启发式来识别哪些包含结构突变、响应时间的突变或前体。 5. 算法 - 比较请求流 5.1 区分时间突变 KS假设检验 - 系统更改前后两个时间段，对于相同的结构，假设检验后时间段是否和前一个时间段相同 非问题期和问题期作为输入；如果测试拒绝假设，则标记为包含响应时间突变 为了识别导致突变的组件或者交互，spectropscope提取critical path-最长时间路径，在此path的edge上也运行假设检验 5.2 区分结构突变 系统在改变前后的请求数类似，则，一个请求数量（在错误阶段）的增加对应着别的请求的减少（采用别的边的请求）。一个分类的请求本身数量也会抖动，所以采用一个阈值。 阈值 – Spectroscope假设在非问题期和问题期运行类似的工作负载。因此，可以合理地预期，在问题期间，通过分布式系统的一条路径的请求数量的增加，应该对应于通过其他路径的请求数量的减少 问题期间的分类，相比非问题之间的分类，如果包含更多的SM_THRESHOLD请求，则标记为突变，包含更少SM_THRESHOLD标记为包含前体（相同请求？相同根节点？） 5.3 前体和突变对应（只有结构突变才有这种对应） 根节点一致 数量限制 剩余前体的请求数量减少 少于 结构突变请求数量增加（根节点一致为前提，且数量变化异常），就删除这个前体 一个前体贡献若干变异，变异的请求数量增加少于前体请求数量减少，排除 字符距离 5.4 排序 权重：突变时间差*数量 如果有多个前体，则使用平均时间加权来排序 5.5 寻找low-level difference 识别突变前后的参数差异，来定位问题 5.6 缺陷 不适用于争用导致的问题 对于不同的负载导致的变化，必须由开发人员确定是否是正常的（不用管） 7. 处理高方差 假设前提是相同时间对应相同路径，所以高方差不可。 8.2 注意：比较一个东西加入前后的差异，得知道前后","link":"/2023/01/12/NSDI11-Spectroscope-Diagnosing-performance-changes-by-comparing-request-flows/"},{"title":"NSDI19 - Zeno - Zeno Diagnosing Performance Problems with Temporal Provenance","text":"总结 提出了时序建模分析，精准刻画请求延时来源 提出了一个工具，给定trace中的两个事件，能刻画事件之间调用链以及为什么第二个事件慢 并不是用来分析系统性能问题，更多的是一种查询显示工具 背景 诊断问题时，定位瓶颈只是第一步。例如定位到网络延时、服务器负载。但正在的问题可能在于通过瓶颈链路发送流量的其他服务，或者通过请求使服务过载的机器。 Intro 现有工具无法解决的问题：假设一台配置错误的机器向存储后端发送大量rpc，该后端过载并延迟了来自其他客户机的请求。 M过多的请求阻塞了真正需要服务的机器C的请求 注意拿咖啡的例子 现有的方法在这种情况下不能满足的原因是它们只关注函数因果关系，它们解释了为什么给定的计算会有某些特定的结果。这种解释只关注计算的直接输入:例如，如果我们想解释一杯咖啡的存在，我们可以关注咖啡豆的来源、杯子的来源和咖啡师的行为。相比之下，时间因果关系也可能涉及其他看似不相关的计算:例如，买咖啡花了太长时间的原因可能是在我们前面等候的顾客太多，而这反过来可能是其他地方的交通堵塞导致大量顾客路过当地商店的结果。同时，在解释延迟时，一些功能依赖性可能被证明是不相关的:例如，即使咖啡豆是制作咖啡所需要的，它们也可能不是造成延迟的原因，因为它们已经在商店中可以买到。 本文更注重于时序因果(temporal causality)，而dapper更注重于功能因果（functional causality) 本文提出一种时序因果的概念和方法，以及如何将其与现有的诊断技术相结合 Overview M对B的请求被称为off-path cause 在google cloud platform中，发现2014年1月至2016年5月，我们选择了所有95个事件报告，既描述了症状，也描述了原因。我们发现超过三分之一(34.7%)的事故是时间错误 传统的trace树 无法确定什么是瓶颈根因 Provenance Provenance：表示一种方法。当操作员要求对某个感兴趣的事件(例如，包的到达)进行解释时，系统可以生成一个递归解释，将事件与一组原因(如包的原始传输和相关路由状态)联系起来。 Approach ① 捕获并简历系统处理请求的顺序 - 定性分析 ② 时间推理和调度理论中的关键路径分析之间的联系 - 时间贡献度计算 - 定量分析 ③ 可视化 Temporal provenance 序列边 dapper的调用关系为因果边，系统调用的顺序为序列边 序列边表示一台机器上事件的处理先后顺序 INS表示事件触发，DRV表示事件处理。黑色线表示因果：A会触发B、C。绿色虚线表示先后，A事件，接着B被处理，接着C被处理。（注意这种表示方法和dapper不一样） 查询 目的：探究请求和响应之间延时的原因 形式：T-Query(e1, e2)，要求e1,e2需要有因果关系 算法 查询经典来源 P:=Query(e2)，则e1一定会包含在p中 识别p中所有 有因果边连接但没有顺序边连接的点(v1,v2)，则v2因为一些原因被延迟(例如上图的C和B，没有因果边，C可能被某些事件延迟) 不断增加路径和事件，最终得到延迟原因 延迟注释：贡献度计算 目的：判断每个子树对整体延时的贡献度 规则：(作者提出三条规则，总结为两条人话) 规则一：整体延时由后向前，递归得分配耗时 A在5s结束，发现B结束用了4s，C结束用了2s，时间递归分配 规则二：无法分配的延时，按照潜在的speed-up分配 难点：A在6s开始，是因为B很慢导致，还是E很慢导致。(X,Y为两台机器) 可以发现，如果B提前，则A不可能提前，因为被E阻塞，要等E返回。但E提前，A会被提前。所以给DRV(B)分配了4s，剩下的B到A的时间，以及E自己的时间，被分给了E请求接收、处理、发送 可视化 为什么可视化：①e1,e2之间事件包含的过多 ②有很多类似的事件可能导致延时。这些会导致整个图看起来很乱 可视化方法 子图分割：递归得，分为若干个事件 分为绿和红，红也要同级事件分割 子图聚合：两个顶点，如果类似则合并 提高可视化 Zeno 作者对RapidNet进行监控分析 作者使用Google dapper 监控一个应用，并通过dapper获取了时序关系和因果关系，在必要的锁争用的地方使用dtrace捕获关系[Q] 作者监控交换机来完成另一个场景的分析 诊断场景 错误的维护任务导致排队 API延迟变高 额外的负载（向每个实例发布更新） 锁争用 作业队列增长 网络拥塞 [Q] 如果dapper的话，事件之间不应该有空格(尤其是作者已经特别标注了网络事件的请求和接收)","link":"/2023/01/11/NSDI19-Zeno-Zeno-Diagnosing-Performance-Problems-with-Temporal-Provenance/"},{"title":"Serverless前沿创新技术","text":"报告方：阿里云 目录 云原生 演进趋势 注意第一代计算单元 serverless 安全问题 软硬一体 基于软硬一体函数计算 wasm技术 serverless异构计算 FaasNet 针推container高突发流量镜像拉取设计 介绍","link":"/2023/01/10/Serverless%E5%89%8D%E6%B2%BF%E5%88%9B%E6%96%B0%E6%8A%80%E6%9C%AF/"},{"title":"SoCC19-Pythia-An automated, cross-layer instrumentation frameworkfor diagnosing performance problems in distributed applications","text":"总结 自动化插桩工具 监控workflow，性能变化会导致workflow的变化，从而确定插桩点 1. Intro 目标：对新出现的性能问题，自动探索能插桩的空间，从而帮助诊断问题 2. Phytia step3: 定位问题组 高变异系数（协方差）或者一直都很慢 一直都很慢：持续缓慢的阈值可以由它的平均响应是否落在所有请求的总体响应时间分布的尾部来确定。 step5: 启用监控 逐层启用：首先是分布式应用程序的服务，然后是服务中的组件，然后是组成组件的节点，然后是节点中的功能","link":"/2023/01/12/SoCC19-Pythia-An-automated-cross-layer-instrumentation-frameworkfor-diagnosing-performance-problems-in-distributed-applications/"},{"title":"SoCC21-Alibaba-Characterizing Microservice Dependency andPerformance: Alibaba Trace Analysis","text":"总结 比较微服务和传统并行任务的区别 从调用图、依赖、运行时性能角度进行分析，同时对动态调用图进行建模 有用的点： 调用图 - 重尾分布 内存CPU影响 - CPU影响更大 - CPU占用越高，RT越高 1. Intro 微服务和传统并行任务调用图不同 重尾分布：10%的调用图有40个以上的微服务 热点 高度动态 有些服务见有强依赖性 UM-DM CPU限制更大 同时RT可以用于拓扑cluster 2. Alibaba trace介绍 通信中，ROC 76%, MQ 23%, IP(进程内通信) 1% 3. 调用图分析 特点 重尾分布 导致传统机器学习来做资源分配是困难的 调用链中，大多数层只包含一个微服务 微服务调用图是高度动态的 聚类结果：一个微服务的调用链中，会有多个类别的调用图/聚类 提出了一个图分类算法 分析 MQ能更少得较少RT 4. 依赖 讲了死锁/回环依赖 但感觉实际上回环依赖比较少 5. 微服务运行时性能 MCR: Microservice call rate (MCR) 一个微服务每分钟被调用的次数 MCR和CPU利用率以及Young GC高度相关，和内存利用率关系不大 只能说明调用高了，CPU利用率就高。 微服务RT性能 CPU占用越高，RT性能越差 6. 动态调用图建模7. 相关工作 有人用系统排队作为指标：网卡、线程、socket，但数据难以获得","link":"/2023/01/12/SoCC21-Alibaba-Characterizing-Microservice-Dependency-andPerformance-Alibaba-Trace-Analysis/"},{"title":"SoCC21-TPROF-Performance profiling via structuralaggregation and automated analysis of distributedsystems traces","text":"trace分析工具 我们的目标是设计新的自动化聚合方法，这些方法利用跟踪结构来支持对性能问题进行更详细的分析。 总结 分布式系统的性能问题分析 通过对trace数据的聚合和分组，来定位系统的缓慢部分，定位最有可能的性能问题 提出了subspan的定义 提出了多个聚合、分组的方法来定位问题 不得不说论文表达有些模棱两可，导致很难理解report图的意思，猜了很久，最后转述了一下 1. Intro 性能问题表现为slowdown trace手动比较很难，用户需要知道正确的轨迹–这也是Spectroscope的缺点 目标是设计新的自动化聚合方法，这些方法利用跟踪结构来支持对性能问题进行更详细的分析。 定位过程 从顶层到底层 时延 – 平均、尾延迟 请求类型 trace结构：缓存命中/未命中 subspan 设计了一个自动工具能够分析结构以及发现可能问题 贡献 设计了一个新的智能运维器，能够聚合trace。在结构的每一层，把trace分组，来进行聚类和分析。低的层被更详细得分组，更详细得分析 提出subspan，使用subspan和span数据来进行分析 2. 相关工作 传统的粗粒度分析器 通过operation排序，损失trace结构信息 通过trace feature分割，损失trace结构信息 通过path聚合，损失延迟信息 累积并计算函数平均时间，不适合发现一小部分变慢（方差问题） 分布式分析耗时很大（数据很多） 3. tprof 回答两个问题 哪些trace应该被分组 哪些数据应该被聚合到一个组中 定义四种聚类分析方法，每个方法有两个步骤 通过trace结构聚类 分析数据 3.2 第1/2层 第一层： 所有trace分为一组，代表整体的行为，看哪些请求最慢，类似之前的粗粒度分析 第二层 按照请求类型、接口进行分组 put/get 分组依据：操作/服务名聚合 分析方法 50th/99th, 平均延迟, 标准差等 也分析 operation self时间 3.3 第3层 分组：trace完全相同(父子结构),但不考虑子结构的顺序。 相比前两层，缓存未命中和查询数据库的跟踪将与缓存命中的跟踪分离 eg: A先调用B再调C和ACB是一样的 可以去除并行导致调用顺序的不同 数据：child_diff [I] 无法考虑调用之间的依赖，第二个依赖于第一个的结束 3.4 第四层 分组：考虑结构和顺序 数据：subspan 分析：同上 3.6 尾延迟 90%可以用于尾延迟判断 3.7 诊断方法（上文分析方法的详细版） 第一层 排序：operation_self * 执行次数进行排序，确定最慢的操作 [I] 没有考虑节点见调用/网络延迟 第二层 分析，是什么原因（从请求类型的角度）导致了第一层中某些操作慢 此外，尾分布定位 tail/normal 第三层 排序：对span执行时间分析 此外，child_diff * 计数，确定跨度最慢部分 第四层 排序：平均subspan持续时间 * 计数 * fraction of time 4 实例 这块很重要，全文的中心思想 report如图所示 !(image-20221101152411-sjnk3vr.png) 得出report的过程 检查所有的Operation，包含所有中间span。发现post-storage-service的服务readPosts的平均自执行时间最慢。 于是有了表格的第一层，这里相当于确定了一个要被诊断的span Request type检查，相当于检查span的所有父span(一个服务接口可能被很多span调用，父span就是一个request type)。发现ReadUserTimeline这个调用可能是尾延迟。（尾延迟是指ReadUserTimeline span在readPosts的所有父span中属于比较慢的一个) 于是有了表格的第二层，这里相当于确定了被诊断的span的父span child检查，相当于检查被诊断span的子调用。发现存在一个subspan很慢，相当于定位根因 展示上面三层 简而言之，一层确定问题span，二层确定其父调用（占问题span的所有父调用的80.3%[Q])，三层确定其子调用/问题所在(占 问题span和其问题父调用共同组成的trace 的73.3%)。 三层共同确定一个trace结构，同时保证问题trace/span尽可能排在图表report的前面 [Q] 此处存疑，原文为80.3% of all traces，是所有trace，还是所有带有问题span的trace？","link":"/2023/01/12/SoCC21-TPROF-Performance-profiling-via-structuralaggregation-and-automated-analysis-of-distributedsystems-traces/"},{"title":"SoCC21-VAIF-Automating instrumentation choices for performanceproblems in distributed applications with VAIF","text":"总结 VAIF: 变化驱动的自动化插桩框架 自动根据性能问题选择需要的插桩点 – 慢代码或不可预期的性能问题(高方差） 本文只需要比较时间即可 （方差大的，延时多的），不比较路径变化，每种路径变化就是一种新的trace。 Pythia的改进版，只看本文即可 问题：只考虑了时间点的变化。 方差反映不足：cache/not cache vs 一两次出问题 路径出现微小变化怎么办 1. Intro 动态日志需要程序员手动指定，搜索空间大 自动化日志注重于错误的记录，非性能问题 性能问题： 功能缓慢、资源争用、负载失衡 VAIF 自动插桩-在正常操作期间，VAIF的操作与今天的分布式跟踪相同，并在默认启用跟踪点级别的情况下生成跟踪。当开发人员必须诊断为什么请求很慢时，他们按下一个按钮，VAIF自动探索必须启用哪些额外的跟踪点来定位问题源。 2. TOWARDS AUTOMATION - 对自动化的一些解释 2.1 log 分为：正确性log和性能问题log 2.2 关键点 相似路径的请求会有相似的时间 相似的工作流，但执行差异大(时间方差大)，则有unknow行为 独立随机变量的方差可直接相加 通过识别在方差中贡献最多的边 来 识别三方库中导致不确定行为的领域 2.3 定位规则 相同的路径但是有高方差，从而锁定贡献最大的边 a - 高方差 低方差但高响应时间 c - 高响应时间 逐步定位，逐步插桩 后面有时间再补","link":"/2023/01/12/SoCC21-VAIF-Automating-instrumentation-choices-for-performanceproblems-in-distributed-applications-with-VAIF/"},{"title":"企业基础架构可观测性Meetup","text":"目录 从可观测性与可交互性视角出发——探讨如何设计出好用的基础软件 如何设计出好用的基础软件 可交互性 控制器 一个电视机一个控制器 文档 quickstart 下一步的交互界面引导 可交互性 多想一步，告诉用户半步，让用户自己走下半部 反馈 不要暴露内部概念!不要暴露内部概念 用精简的人话暴露进展和状态 不要说废话，更不要不说反馈必须即时 配置 可观测性 直觉 分布式链路追踪在字节跳动的实践 metric/log/trace trace：metric和log关联的纽带 实现 trace等定义类似CNCF opentracing 关联关系 埋点： 日志基础库 采样 PostTrace后置采样 个人理解就是记录下来，相关的trace都会被涉及到 一种是通过消息分析 一种是通过log记录下来，然后总结log，分析出trace（后置采样） 整体架构 私有数据流：只解码部分需要的header 容灾时的高可用，这种情况下追踪更需要‘ 分析计算 效果 实践 错误追踪/错误传播链分析 一些思路 除了系统内数据，还有网络、CPU调用的数据 阿里十年链路追踪与应用可观测实践 更多相关落地实践遇到的问题 架构推演 挑战 成本收益：存储(5PB+)、网络 最后一公里：链路追踪无法定位问题 探针管理 pandora: classloader、双亲委派，类似于一个SDK 无侵入，ARMS，基于pinpoint 成本收益 5-30分钟内才是最有价值可能被用户使用的数据 冷热数据分离 预聚合：客户端聚合，这样即使调用链采样发送，聚合的数据是准确的 最后一公里 方法栈追踪，自动化 告警 扩展（赋能） 业务场景为中心的染色链路 类似后置采样，收集全部链路后再染色 RASP 趋势 注意右下角几个社区 全景监控 QA 方法栈追踪 入的世界、出的时间，如果进来的时间大于1s，就启动一个线程去抓方法栈 1入，5出，抓2-5 如何低代价追踪，就是产品竞争力 注意那几个平台社区 目前对Java插桩，go等不支持 数据预处理，客户端。数据热处理，边缘节点计算 网易云原生日志平台的架构演进与实践（轻舟日志） 不是链路，更偏向日志 目录 日志采集 sidecar sidecat vs daemonSet 实现：ripple+agent filebeat 架构 遇到的问题 总览 新版agent/解决 基于CRD和K8S的使用 eBay 云原生生产环境下的日志监控 云原生环境下的日志系统","link":"/2023/01/10/%E4%BC%81%E4%B8%9A%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7Meetup/"},{"title":"基于eBPF的Istio服务网格数据面网络优化","text":"作者 杨婉琪等 中山大学 背景 优化方法 时延分布 优化架构 简而言之，把k8s pod到代理的流量部分给优化掉，直接重定向","link":"/2023/01/10/%E5%9F%BA%E4%BA%8EeBPF%E7%9A%84Istio%E6%9C%8D%E5%8A%A1%E7%BD%91%E6%A0%BC%E6%95%B0%E6%8D%AE%E9%9D%A2%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/"},{"title":"基于知识图谱的云原生系统故障检测与定位方法研究","text":"作者 陈彩琳等 中山大学 研究背景 级联失效 研究问题 研究系统 知识图谱构建 只用K8S API 不需要插桩tracing 构建网络依赖 [todo] netstat 结果 构建关系 – 知识图谱-实体依赖关系 异常检测 属性图的异常检测算法 算法细节 根因定位 引入trace信息","link":"/2023/01/10/%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E4%BA%91%E5%8E%9F%E7%94%9F%E7%B3%BB%E7%BB%9F%E6%95%85%E9%9A%9C%E6%A3%80%E6%B5%8B%E4%B8%8E%E5%AE%9A%E4%BD%8D%E6%96%B9%E6%B3%95%E7%A0%94%E7%A9%B6/"},{"title":"基于系统特征的微服务拆分框架","text":"作者 曹伶俐 安徽大学 研究现状 研究内容 整体框架 [todo] 社区检测算法，多目标优化 代码 特征提取 社区检测 多目标优化 实验设置 拆分算法","link":"/2023/01/10/%E5%9F%BA%E4%BA%8E%E7%B3%BB%E7%BB%9F%E7%89%B9%E5%BE%81%E7%9A%84%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%8B%86%E5%88%86%E6%A1%86%E6%9E%B6/"},{"title":"微服务故障检测研究综述","text":"作者 王璐 西安电子科技大学 目录 方案与概括 研究方向 故障分类 产业 现状 监控 [todo] istio能获取数据？被动跟踪，能到什么粒度 故障诊断 故障预测 未来 整体正反反馈，整体一体化 介绍","link":"/2023/01/10/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%95%85%E9%9A%9C%E6%A3%80%E6%B5%8B%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0/"},{"title":"软件学报17-一种基于执行轨迹监测的微服务故障诊断方法","text":"异常诊断+根因定位异常诊断： 利用调用树对请求处理的执行轨迹进行刻画 针对影响执行轨迹的系统故障,利用树编辑距离来评估请求处理的异常程度 根因定位 通过分析执行轨迹差异来定位引发故障的方法调用 针对性能异常,采用主成分分析抽取引起系统性能异常波动的关键方法调用.","link":"/2023/01/12/%E8%BD%AF%E4%BB%B6%E5%AD%A6%E6%8A%A517-%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%E6%89%A7%E8%A1%8C%E8%BD%A8%E8%BF%B9%E7%9B%91%E6%B5%8B%E7%9A%84%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%95%85%E9%9A%9C%E8%AF%8A%E6%96%AD%E6%96%B9%E6%B3%95/"},{"title":"面向大数据与机器学习的无服务器计算研究综述","text":"无服务计算特点 程序之间不能通信 状态存储到外部存储 挑战 相关工作 numpywren 矩阵计算 总结 展望","link":"/2023/01/10/%E9%9D%A2%E5%90%91%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%97%A0%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%AE%A1%E7%AE%97%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0/"}],"tags":[{"name":"微服务","slug":"微服务","link":"/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"智能运维","slug":"智能运维","link":"/tags/%E6%99%BA%E8%83%BD%E8%BF%90%E7%BB%B4/"},{"name":"Fuzzing","slug":"Fuzzing","link":"/tags/Fuzzing/"},{"name":"REST API","slug":"REST-API","link":"/tags/REST-API/"},{"name":"测试","slug":"测试","link":"/tags/%E6%B5%8B%E8%AF%95/"},{"name":"性能分析","slug":"性能分析","link":"/tags/%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"},{"name":"测试生成","slug":"测试生成","link":"/tags/%E6%B5%8B%E8%AF%95%E7%94%9F%E6%88%90/"},{"name":"Profiling","slug":"Profiling","link":"/tags/Profiling/"},{"name":"故障分析","slug":"故障分析","link":"/tags/%E6%95%85%E9%9A%9C%E5%88%86%E6%9E%90/"},{"name":"Serverless","slug":"Serverless","link":"/tags/Serverless/"},{"name":"可观测性","slug":"可观测性","link":"/tags/%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/"},{"name":"Istio","slug":"Istio","link":"/tags/Istio/"},{"name":"eBPF","slug":"eBPF","link":"/tags/eBPF/"},{"name":"云原生","slug":"云原生","link":"/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/"}],"categories":[{"name":"论文笔记","slug":"论文笔记","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"},{"name":"微服务智能运维","slug":"论文笔记/微服务智能运维","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%99%BA%E8%83%BD%E8%BF%90%E7%BB%B4/"},{"name":"REST API Fuzzing","slug":"论文笔记/REST-API-Fuzzing","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/REST-API-Fuzzing/"},{"name":"会议纪要","slug":"会议纪要","link":"/categories/%E4%BC%9A%E8%AE%AE%E7%BA%AA%E8%A6%81/"},{"name":"CCF2021-云原生软件技术与工程实践论坛","slug":"会议纪要/CCF2021-云原生软件技术与工程实践论坛","link":"/categories/%E4%BC%9A%E8%AE%AE%E7%BA%AA%E8%A6%81/CCF2021-%E4%BA%91%E5%8E%9F%E7%94%9F%E8%BD%AF%E4%BB%B6%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E8%AE%BA%E5%9D%9B/"}],"pages":[{"title":"about","text":"","link":"/about/index.html"}]}